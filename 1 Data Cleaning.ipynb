{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597821269895",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "> Data cleaning helps avoid \"garbage in , garbage out\" -- we do not want to feed meaningless data into a model which will probably return us with more meaningless junk.\n",
    "\n",
    "This time I will skip the scraping part that data scientists normally do. This allows the content to be updated over time, but to be fair the content is pretty static anyways so I don't really see the point of doing so. In addition, I imagine there would be quite a number of problems if the layout of the site changes.\n",
    "\n",
    "## Outline for data cleaning\n",
    "- Input: a simple text file with metadata removed. Headers and page numbers are kept though.\n",
    "- Common Pre-processing/ cleaning procedures\n",
    "  - All lower case\n",
    "  - Remove punctuation, symbols and numerical values\n",
    "  - Remove common non-sensical text (such as line breakers `\\n`)\n",
    "  - Tokenize text: split sentences into individual words (in preparation for DTM)\n",
    "  - Remove stop-words\n",
    "  - Using NLTK perform stemming and lemmatisation for words in the DTM, to reduce the number of inflicted words.\n",
    "  - Parts of speech tagging\n",
    "  - DTM for bi-grams/ tri-grams (phrases like thank you)\n",
    "- Output\n",
    "  - Corpus: not much different from the actual input since there is only one file here....... but with all the data cleaned up.\n",
    "  - Document Term matrix: a matrix of word counts in the entire corpus.\n",
    "\n",
    "SpaCy can also perform these NLTK techniques as well, with a greater degree of efficiency. The extra features might be overkill for the time being though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "\n",
    "For the purposes of this project, I will simply import the data from a text file, which will be parsed into a string object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Define list of file names for books to be analysed\n",
    "fileNames = [\n",
    "    'books/charlieandthechocolatefactory.txt',\n",
    "    'books/fantasticmrfox.txt',\n",
    "    'books/matilda.txt'\n",
    "]\n",
    "\n",
    "# bookNames for indexing the dictionary in later steps\n",
    "bookNames = [\n",
    "    'chocofact',\n",
    "    'fox',\n",
    "    'matilda'\n",
    "]\n",
    "\n",
    "# fullNames available for better readability\n",
    "fullNames = [\n",
    "    'Charlie and the Chocolate Factory',\n",
    "    'Fantastic Mr Fox!',\n",
    "    'Matilda'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text files and remove non sensical '\\n'\n",
    "def importText(fileName):\n",
    "    data = open(fileName, \"r\", encoding=\"utf-8\").read().replace('\\n', ' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The Reader of Books    It’s a funny thing about mothers and fathers. Even  when their own child is the most disgusting little blister  you could ever imagine, they still think that he or she is  wonderful.   Some parents go further. They become so blinded by  adoration they manage to convince themselves their  child has qualities of genius.   Well, there is nothing very wrong with all this. It’s the  way of the world. It is only when the parents begin  telling us about the brilliance of their own revolting off¬  spring, that we start shouting, 'Bring us a basin! We’re  going to be sick!’    3      School teachers suffer a good deal from having to  listen to this sort of twaddle from proud parents, but  they usually get their own back when the time comes  to write the end-of-term reports. If I were a teacher I  would cook up some real scorchers for the children of  doting parents. ‘Your son Maximilian,’ I would write,  ‘is a total wash-out. I hope you have a family business  you can push him into when he leaves school because  he sure as heck won’t get a job anywhere else.’ Or if I  were feeling lyrical that day, 1 might write, ‘It is a  curious truth that grasshoppers have their hearing-  organs in the sides of the abdomen. Your daughter  Vanessa, judging by what she’s learnt this term, has no  hearing-organs at all.’    4          I might even delve deeper into natural history and  say, ‘The periodical cicada spends six years as a grub  underground, and no more than six days as a free  creature of sunlight and air. Your son Wilfred has spent  six years as a grub in this school and we are still  waiting for him to emerge from the chrysalis. 5 A  particularly poisonous little girl might sting me into  saying, ‘Fiona has the same glacial beauty as an  iceberg, but unlike the iceberg she has absolutely     nothing below the surface. 5 I think I might enjoy  writing end-of-term reports for the stinkers in my class.  But enough of that. We have to get on.    5          Occasionally one conies across parents who take the  opposite line, who show no interest at all in their  children, and these of course are far worse than the  doting ones. Mr and Mrs Wormwood were two such  parents. They had a son called Michael and a  1 /> daughter called Matilda, and the parents  . , O looked upon Matilda in particular as  Of ^ ^ K, nothin? more than a scab. A scab is some-  Vas A thing you have to put up with until the  time comes when you can pick it off and flick  it away. Mr and Mrs Wormwood looked forward  enormously to the time when they could pick  their little daughter off and flick her away,  preferably into the next county 7 or even further  than that.   It is bad enough when parents treat ordinary  children as though they were scabs and bunions, but it  becomes somehow a lot worse when the child  in question is extra- ordinary, and by that I  mean sensitive and brilliant. Matilda was  both of these things, but above all she was  brilliant. Her mind was so\n"
    }
   ],
   "source": [
    "# Test print the first 5000 characters of the third book, which is 'Matilda'\n",
    "rawBooks = [importText(bkName) for bkName in fileNames]\n",
    "print(rawBooks[2][:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we want to make sure that the texts are indexed with their short-hand names as well, such that we don't necessarily have to access them with a specific number. This makes it way more convenient the access the data in the future, especially if we decide to append a few more copies of Roald Dahl's texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   full_names  \\\nbook_names                                      \nchocofact   Charlie and the Chocolate Factory   \nfox                         Fantastic Mr Fox!   \nmatilda                               Matilda   \n\n                                                                                                                                                             text  \nbook_names                                                                                                                                                         \nchocofact   1   Here Comes Charlie   These two very old people are the father and mother of Mr Bucket. Their names are  Grandpa Joe and Grandma Josephine.   A...  \nfox         Down in the valley there were three farms. The owners of these farms had done well. They were rich men. They were also nasty men. All three of the...  \nmatilda      The Reader of Books    It’s a funny thing about mothers and fathers. Even  when their own child is the most disgusting little blister  you could ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_names</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>Charlie and the Chocolate Factory</td>\n      <td>1   Here Comes Charlie   These two very old people are the father and mother of Mr Bucket. Their names are  Grandpa Joe and Grandma Josephine.   A...</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>Fantastic Mr Fox!</td>\n      <td>Down in the valley there were three farms. The owners of these farms had done well. They were rich men. They were also nasty men. All three of the...</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>Matilda</td>\n      <td>The Reader of Books    It’s a funny thing about mothers and fathers. Even  when their own child is the most disgusting little blister  you could ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "raw_df = pd.DataFrame({'book_names':bookNames, 'full_names': fullNames, 'text':rawBooks})\n",
    "\n",
    "# set book names as index\n",
    "raw_df = raw_df.set_index('book_names')\n",
    "\n",
    "# sort dataframe and print\n",
    "raw_df = raw_df.sort_index()\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed any further, we should also pickle a raw copy of all books, which saves the object in a binary format. This is done for contingency purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"raw_books.pkl\", \"wb\") as file:\n",
    "    pickle.dump(raw_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the mean time, it is also a good idea to quickly verify the data structure (such as the headers), as well as the text, which are more or less unmodified at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<bound method NDFrame.keys of                                    full_names  \\\nbook_names                                      \nchocofact   Charlie and the Chocolate Factory   \nfox                         Fantastic Mr Fox!   \nmatilda                               Matilda   \n\n                                                                                                                                                             text  \nbook_names                                                                                                                                                         \nchocofact   1   Here Comes Charlie   These two very old people are the father and mother of Mr Bucket. Their names are  Grandpa Joe and Grandma Josephine.   A...  \nfox         Down in the valley there were three farms. The owners of these farms had done well. They were rich men. They were also nasty men. All three of the...  \nmatilda      The Reader of Books    It’s a funny thing about mothers and fathers. Even  when their own child is the most disgusting little blister  you could ...  >"
     },
     "metadata": {},
     "execution_count": 300
    }
   ],
   "source": [
    "# Check the list of book names (keys)\n",
    "raw_df.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' cider inside her inside.’  Then Badger joined in: 48  \\x0c‘Oh poor Mrs Badger, he cried, So hungry she very near died. But she’ll not feel so hollow If only she’ll swallow Some cider inside her inside.’  They were still singing as they rounded the final corner and burst in upon the most wonderful and amazing sight any of them had ever seen. The feast was just beginning. A large dining-room had been hollowed out of the earth, and in the middle of it, seated around a huge table, were no less than twenty-nine animals. They were: Mrs Fox and three Small Foxes. Mrs Badger and three Small Badgers. Mole and Mrs Mole and four Small Moles. Rabbit and Mrs Rabbit and five Small Rabbits. 49  \\x0cWeasel and Mrs Weasel and six Small Weasels. The table was covered with chickens and ducks and geese and hams and bacon, and everyone was tucking into the lovely food. ‘My darling!’ cried Mrs Fox, jumping up and hugging Mr Fox. ‘We couldn’t wait! Please forgive us!’ Then she hugged the Smallest Fox of all, and Mrs Badger hugged Badger, and everyone hugged everyone else. Amid shouts of joy, the great jars of cider were placed upon the table, and Mr Fox and Badger and the Smallest Fox sat down with the others. You must remember no one had eaten a thing for several days. They were ravenous. So for a while there was no conversation at all. There was only the sound of crunching and chewing as the animals attacked the succulent food. At last, Badger stood up. He raised his glass of cider and called out, ‘A toast! I want you all to stand and drink a toast to our dear friend who has saved our lives this day – Mr Fox!’ ‘To Mr Fox!’ they all shouted, standing up and raising their glasses. ‘To Mr Fox! Long may he live!’ Then Mrs Fox got shyly to her feet and said, ‘I don’t want to make a speech. I just want to say one thing, and it is this: MY HUSBAND IS A FANTASTIC FOX.’ 50  \\x0cEveryone clapped and cheered. Then Mr Fox himself stood up. ‘This delicious meal . . .’ he began, then he stopped. In the silence that followed, he let fly a tremendous belch. There was laughter and more clapping. ‘This delicious meal, my friends,’ he went on, ‘is by courtesy of Messrs Boggis, Bunce and Bean.’ (More cheering and laughter.) ‘And I hope you have enjoyed it as much as I have.’ He let fly another colossal belch. ‘Better out than in,’ said Badger. ‘Thank you,’ said Mr Fox, grinning hugely. ‘But now, my friends, let us be serious. Let us think of tomorrow and the next day and the days after that. If we go out, we will be killed. Right?’ ‘Right!’ they shouted. ‘We’ll be shot before we’ve gone a yard,’ said Badger. ‘Ex-actly,’ said Mr Fox. ‘But whowants to go out, anyway; let me ask you that? We are all diggers, every one of us. We hate the outside. The outside is full of enemies. We only go out because we have to, to get food for our families. But now, my friends, we have an entirely new set-up. We have a safe tunnel leading to three of the finest stores in the world!’ ‘We do indeed!’ said Badger. ‘I’ve seen ’em!’ 51  \\x0c‘And you know what this means?’ said Mr Fox. ‘It means that none of us need ever go out into the open again!’ There was a buzz of excitement around the table. ‘I therefore invite you all,’ Mr Fox went on, ‘to stay here with me for ever.’ ‘For ever!’ they cried. ‘My goodness! How marvellous!’ And Rabbit said to Mrs Rabbit, ‘My dear, just think! We’re never going to be shot at again in our lives!’ ‘We will make,’ said Mr Fox, ‘a little underground village, with streets and houses on each side – separate houses for Badgers and Moles and Rabbits and Weasels and Foxes. And every day I will go shopping for you all. And every day we will eat like kings.’ The cheering that followed this speech went on for many minutes.  52  \\x0cOutside the fox’s hole, Boggis and Bunce and Bean sat beside their tents with their guns on their laps. It was beginning to rain. Water was trickling down the necks of the three men and into their shoes. ‘He won’t stay down there much longer now,’ Boggis said. ‘The brute must be famished,’ Bunce said. ‘That’s right,’ Bean said. ‘He’ll be making a dash for it any moment. Keep your guns handy.’ They sat there by the hole, waiting for the fox to come out. And so far as I know, they are still waiting.  53  \\x0c'"
     },
     "metadata": {},
     "execution_count": 301
    }
   ],
   "source": [
    "# Test print the contents for Fantastic Mr Fox!\n",
    "raw_df.text.loc['fox'][47000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Data cleaning!\n",
    "\n",
    "When data scientists process numerical data, they often remove invalid data (which can be automatically and manually interpreted), duplicate data, outliers and null data. There are several methods that we can iteratively apply along the way to clean our data:\n",
    "  - All lower case\n",
    "  - Remove punctuation, symbols and numerical values\n",
    "  - Remove common non-sensical text (such as line breakers `\\n`, as well as other escape characters such as `51\\x0c`)\n",
    "  - Tokenize text: split sentences into individual words (in preparation for DTM)\n",
    "  - Remove stop-words\n",
    "  - Using NLTK perform stemming and lemmatisation for words in the DTM, to reduce the number of inflicted words.\n",
    "  - Parts of speech tagging\n",
    "  - DTM for bi-grams/ tri-grams (phrases like thank you)\n",
    "  - fix typos (a bit too advanced.......)\n",
    "\n",
    "We want to apply these methods iteratively such that we can observe the results after each cleaning stage; this is especially important for text-preprocessing since an overly aggressive approach may result in key information being lost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make all text lower case, get rid of punctuation, numbers and other non-sensical text.\n",
    "\n",
    "# Packages for string manipulation\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Basic Function for cleaning texts\n",
    "def basic_text_clean(text):\n",
    "    text = text.lower() #lower case\n",
    "    text = re.sub('\\x0c', ' ', text) # non sensical text\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text) # punctuation\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text) # numbers in between text\n",
    "    return text\n",
    "\n",
    "basic_cleaning = lambda x: basic_text_clean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some say quotes (things within single or double quotation marks) should be removed as well, but in this context, I believe dialogues or conversations within the story are pretty important as well, so might as well see how things work out first......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                                                                                                                             text\nbook_names                                                                                                                                                       \nchocofact       here comes charlie   these two very old people are the father and mother of mr bucket  their names are  grandpa joe and grandma josephine    a...\nfox         down in the valley there were three farms  the owners of these farms had done well  they were rich men  they were also nasty men  all three of the...\nmatilda      the reader of books    it’s a funny thing about mothers and fathers  even  when their own child is the most disgusting little blister  you could ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>here comes charlie   these two very old people are the father and mother of mr bucket  their names are  grandpa joe and grandma josephine    a...</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>down in the valley there were three farms  the owners of these farms had done well  they were rich men  they were also nasty men  all three of the...</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>the reader of books    it’s a funny thing about mothers and fathers  even  when their own child is the most disgusting little blister  you could ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 303
    }
   ],
   "source": [
    "# Apply the basic_cleaning \"mask\" to the texts within the dataframe\n",
    "data_b_cleaned = pd.DataFrame(raw_df.text.apply(basic_cleaning))\n",
    "data_b_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'r joined in      ‘oh poor mrs badger  he cried  so hungry she very near died  but she’ll not feel so hollow if only she’ll swallow some cider inside her inside ’  they were still singing as they rounded the final corner and burst in upon the most wonderful and amazing sight any of them had ever seen  the feast was just beginning  a large dining room had been hollowed out of the earth  and in the middle of it  seated around a huge table  were no less than twenty nine animals  they were  mrs fox and three small foxes  mrs badger and three small badgers  mole and mrs mole and four small moles  rabbit and mrs rabbit and five small rabbits      weasel and mrs weasel and six small weasels  the table was covered with chickens and ducks and geese and hams and bacon  and everyone was tucking into the lovely food  ‘my darling ’ cried mrs fox  jumping up and hugging mr fox  ‘we couldn’t wait  please forgive us ’ then she hugged the smallest fox of all  and mrs badger hugged badger  and everyone hugged everyone else  amid shouts of joy  the great jars of cider were placed upon the table  and mr fox and badger and the smallest fox sat down with the others  you must remember no one had eaten a thing for several days  they were ravenous  so for a while there was no conversation at all  there was only the sound of crunching and chewing as the animals attacked the succulent food  at last  badger stood up  he raised his glass of cider and called out  ‘a toast  i want you all to stand and drink a toast to our dear friend who has saved our lives this day – mr fox ’ ‘to mr fox ’ they all shouted  standing up and raising their glasses  ‘to mr fox  long may he live ’ then mrs fox got shyly to her feet and said  ‘i don’t want to make a speech  i just want to say one thing  and it is this  my husband is a fantastic fox ’     everyone clapped and cheered  then mr fox himself stood up  ‘this delicious meal      ’ he began  then he stopped  in the silence that followed  he let fly a tremendous belch  there was laughter and more clapping  ‘this delicious meal  my friends ’ he went on  ‘is by courtesy of messrs boggis  bunce and bean ’  more cheering and laughter   ‘and i hope you have enjoyed it as much as i have ’ he let fly another colossal belch  ‘better out than in ’ said badger  ‘thank you ’ said mr fox  grinning hugely  ‘but now  my friends  let us be serious  let us think of tomorrow and the next day and the days after that  if we go out  we will be killed  right ’ ‘right ’ they shouted  ‘we’ll be shot before we’ve gone a yard ’ said badger  ‘ex actly ’ said mr fox  ‘but whowants to go out  anyway  let me ask you that  we are all diggers  every one of us  we hate the outside  the outside is full of enemies  we only go out because we have to  to get food for our families  but now  my friends  we have an entirely new set up  we have a safe tunnel leading to three of the finest stores in the world ’ ‘we do indeed ’ said badger  ‘i’ve seen ’em ’     ‘and you know what this means ’ said mr fox  ‘it means that none of us need ever go out into the open again ’ there was a buzz of excitement around the table  ‘i therefore invite you all ’ mr fox went on  ‘to stay here with me for ever ’ ‘for ever ’ they cried  ‘my goodness  how marvellous ’ and rabbit said to mrs rabbit  ‘my dear  just think  we’re never going to be shot at again in our lives ’ ‘we will make ’ said mr fox  ‘a little underground village  with streets and houses on each side – separate houses for badgers and moles and rabbits and weasels and foxes  and every day i will go shopping for you all  and every day we will eat like kings ’ the cheering that followed this speech went on for many minutes       outside the fox’s hole  boggis and bunce and bean sat beside their tents with their guns on their laps  it was beginning to rain  water was trickling down the necks of the three men and into their shoes  ‘he won’t stay down there much longer now ’ boggis said  ‘the brute must be famished ’ bunce said  ‘that’s right ’ bean said  ‘he’ll be making a dash for it any moment  keep your guns handy ’ they sat there by the hole  waiting for the fox to come out  and so far as i know  they are still waiting       '"
     },
     "metadata": {},
     "execution_count": 304
    }
   ],
   "source": [
    "data_b_cleaned.text.loc['fox'][47000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus cleaned!\n",
    "\n",
    "A quick round-up of things we have worked on:\n",
    "- Indexed this list of texts with keys, which are the book names - at this point, this can be considered a corpus, map of book names to texts.\n",
    "- As we can see, not much has actually been done so far, but most importantly the text makes sense and the flow of sentences is not disturbed by weird characters. \n",
    "- Since the actual order of words do matter for things like sentiment analysis, performing further cleaning techniques such as stemming and lemmatisation will only worsen the final sentence generation algorithm. \n",
    "\n",
    "The corpus is ready to be pickled for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b_cleaned.to_pickle(\"basic_cleaned_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Document Term Matrix (DTM) with NLTK\n",
    "\n",
    "## Tokenization and Stop Words\n",
    "\n",
    "A DTM is a database of words within various documents. This means the text is split into words (tokenization) for further analysis. In this sceario, we can apply further techqniques to remove  stop words, which are relatively meaningless words, such as 'a', 'the' or other various prepositions.\n",
    "\n",
    "The pakages that we can utilise for this section are:\n",
    "- scikit-learn's CountVectorizer\n",
    "- NLTK\n",
    "- (maybe) SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            aback  abandon  abc  abdomen  abide  abilities  ability  able  \\\nbook_names                                                                  \nchocofact       0        0    0        0      1          0        0    13   \nfox             0        0    0        0      0          0        0     0   \nmatilda         1        1    1        1      0          1        3    14   \n\n            absolute  absolutely  ...  yippeeeeee  yippeeeeeeee  \\\nbook_names                        ...                             \nchocofact          2          10  ...           1             1   \nfox                0           1  ...           0             0   \nmatilda            5           6  ...           0             0   \n\n            yippeeeeeeeeee  youheard  young  younger  youreally  youth  zing  \\\nbook_names                                                                     \nchocofact                1         0      6        1          0      1     1   \nfox                      0         1      1        0          1      0     0   \nmatilda                  0         0     14        1          0      0     0   \n\n            zip  \nbook_names       \nchocofact     1  \nfox           0  \nmatilda       0  \n\n[3 rows x 5651 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aback</th>\n      <th>abandon</th>\n      <th>abc</th>\n      <th>abdomen</th>\n      <th>abide</th>\n      <th>abilities</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>absolute</th>\n      <th>absolutely</th>\n      <th>...</th>\n      <th>yippeeeeee</th>\n      <th>yippeeeeeeee</th>\n      <th>yippeeeeeeeeee</th>\n      <th>youheard</th>\n      <th>young</th>\n      <th>younger</th>\n      <th>youreally</th>\n      <th>youth</th>\n      <th>zing</th>\n      <th>zip</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>2</td>\n      <td>10</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>14</td>\n      <td>5</td>\n      <td>6</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 5651 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 306
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Tokenize text, fit it into a mask (that removes stop words), and transform it in a DTM\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_b_cleaned.text) \n",
    "\n",
    "#Transform DTM (in context a vocabulary/ dictionary) into pandas dataframe format\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()) \n",
    "data_dtm.index = data_b_cleaned.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of this format are immediately apparent. It allows us to filter some meaningless words (stop words) and presents the data in a neat and organised manner. Let's pickle this relatively raw or primitive DTM first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.to_pickle(\"data_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "Dealing with an excessively large dataset is messy for various reasons:\n",
    "- As we can see, there are some really similar words like \"young\" or \"younger\", which are stemmings or lemmatisations from a \"root word\"\n",
    "- Some words that have arbitrary spelling like \"yippeeeeeeee....!\" -- it's cute, but who knows why Roald Dahl likes to put `x` many e's behind yippee......\n",
    "- Frequency for certain words are quite low, which means that they won't really be used for things like topic analysis or sentiment analysis in later stages. Perhaps identifying unique words may be important, but for these purposes I have specifically pickled the DTM above into the file `data_dtm.pkl`; or we could always start off from the corpus.\n",
    "\n",
    "To recall, these are the things that we can work on to further clean our data:\n",
    "  - Perform stemming and lemmatisation for words in the DTM, to reduce the number of inflicted words.\n",
    "  - Parts of speech tagging\n",
    "  - Collating bi-grams/ tri-grams (phrases like thank you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            aback  aback arrival  abandon  abandon trunchbull  abc  abc quite  \\\nbook_names                                                                      \nchocofact       0              0        0                   0    0          0   \nfox             0              0        0                   0    0          0   \nmatilda         1              1        1                   1    1          1   \n\n            abdomen  abdomen daughter  abide  abide ugliness  ...  \\\nbook_names                                                    ...   \nchocofact         0                 0      1               1  ...   \nfox               0                 0      0               0  ...   \nmatilda           1                 1      0               0  ...   \n\n            younger children  younger ones  youreally  youreally mean  youth  \\\nbook_names                                                                     \nchocofact                  0             1          0               0      1   \nfox                        0             0          1               1      0   \nmatilda                    1             0          0               0      0   \n\n            youth just  zing  zing fantastic  zip  zip guns  \nbook_names                                                   \nchocofact            1     1               1    1         1  \nfox                  0     0               0    0         0  \nmatilda              0     0               0    0         0  \n\n[3 rows x 34892 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aback</th>\n      <th>aback arrival</th>\n      <th>abandon</th>\n      <th>abandon trunchbull</th>\n      <th>abc</th>\n      <th>abc quite</th>\n      <th>abdomen</th>\n      <th>abdomen daughter</th>\n      <th>abide</th>\n      <th>abide ugliness</th>\n      <th>...</th>\n      <th>younger children</th>\n      <th>younger ones</th>\n      <th>youreally</th>\n      <th>youreally mean</th>\n      <th>youth</th>\n      <th>youth just</th>\n      <th>zing</th>\n      <th>zing fantastic</th>\n      <th>zip</th>\n      <th>zip guns</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 34892 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "source": [
    "# Accept bigrams as well\n",
    "bigrams_cv = CountVectorizer(stop_words='english', ngram_range= (1,2)) \n",
    "cleaned_cv = bigrams_cv.fit_transform(data_b_cleaned.text) \n",
    "cleaned_data_dtm = pd.DataFrame(cleaned_cv.toarray(), columns=bigrams_cv.get_feature_names())\n",
    "cleaned_data_dtm.index = data_dtm.index\n",
    "cleaned_data_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imagine the DTM is quite sparse since there are 35000 columns! Not only is most of the data useless, it will also increase the load for processing in subsequent steps. Let's only keep words that have appeared more than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "book_names          chocofact  fox  matilda  sum\naback                       0    0        1    1\naback arrival               0    0        1    1\nabandon                     0    0        1    1\nabandon trunchbull          0    0        1    1\nabc                         0    0        1    1\n...                       ...  ...      ...  ...\nyouth just                  1    0        0    1\nzing                        1    0        0    1\nzing fantastic              1    0        0    1\nzip                         1    0        0    1\nzip guns                    1    0        0    1\n\n[34892 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>book_names</th>\n      <th>chocofact</th>\n      <th>fox</th>\n      <th>matilda</th>\n      <th>sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>aback</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>aback arrival</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>abandon</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>abandon trunchbull</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>abc</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>youth just</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>zing</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>zing fantastic</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>zip</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>zip guns</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>34892 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 309
    }
   ],
   "source": [
    "# Transpose for easier calculations, and add a column for number for sum of word frequencies in all books.\n",
    "cleaned_data_dtm = cleaned_data_dtm.transpose()\n",
    "cleaned_data_dtm['sum'] = cleaned_data_dtm.sum(axis = 1, skipna = True) \n",
    "cleaned_data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            able  absolute  absolutely  actually  added  afraid  afternoon  \\\nbook_names                                                                   \nchocofact     13         2          10         5      3       3          2   \nfox            0         0           1         1      0       0          1   \nmatilda       14         5           6         9      9       8         15   \nsum           27         7          17        15     12      11         18   \n\n            afternoons  age  ago  ...  year old  years  yelled  yelled mrs  \\\nbook_names                        ...                                        \nchocofact            0    0    2  ...         2      8      14           7   \nfox                  0    0    0  ...         0      0       4           0   \nmatilda              9    7    7  ...        11     23      13           0   \nsum                  9    7    9  ...        13     31      31           7   \n\n            yelling  yellow  yes  yes miss  yesterday  young  \nbook_names                                                    \nchocofact         3       3   29         0          2      6  \nfox               0       0   11         0          1      1  \nmatilda           4       4   37        15          6     14  \nsum               7       7   77        15          9     21  \n\n[4 rows x 1353 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>absolute</th>\n      <th>absolutely</th>\n      <th>actually</th>\n      <th>added</th>\n      <th>afraid</th>\n      <th>afternoon</th>\n      <th>afternoons</th>\n      <th>age</th>\n      <th>ago</th>\n      <th>...</th>\n      <th>year old</th>\n      <th>years</th>\n      <th>yelled</th>\n      <th>yelled mrs</th>\n      <th>yelling</th>\n      <th>yellow</th>\n      <th>yes</th>\n      <th>yes miss</th>\n      <th>yesterday</th>\n      <th>young</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>13</td>\n      <td>2</td>\n      <td>10</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2</td>\n      <td>8</td>\n      <td>14</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>29</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>14</td>\n      <td>5</td>\n      <td>6</td>\n      <td>9</td>\n      <td>9</td>\n      <td>8</td>\n      <td>15</td>\n      <td>9</td>\n      <td>7</td>\n      <td>7</td>\n      <td>...</td>\n      <td>11</td>\n      <td>23</td>\n      <td>13</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>37</td>\n      <td>15</td>\n      <td>6</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>sum</th>\n      <td>27</td>\n      <td>7</td>\n      <td>17</td>\n      <td>15</td>\n      <td>12</td>\n      <td>11</td>\n      <td>18</td>\n      <td>9</td>\n      <td>7</td>\n      <td>9</td>\n      <td>...</td>\n      <td>13</td>\n      <td>31</td>\n      <td>31</td>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n      <td>77</td>\n      <td>15</td>\n      <td>9</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 1353 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "source": [
    "# Only keep words that have appeared more than 5 times.\n",
    "filtered_data_dtm = cleaned_data_dtm[cleaned_data_dtm['sum'] > 5]\n",
    "\n",
    "#Transpose back to the original format and display the DTM!\n",
    "filtered_data_dtm = filtered_data_dtm.transpose()\n",
    "filtered_data_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields us with a much more manageable 1361 entries. Before we proceed any furher, we should again pickle it for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.to_pickle(\"high_bigram_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Stemming and Lemmatisation on NLTK\n",
    "\n",
    "When a language contains words that are derived from another (root) word , the changes are called **inflected Language**. For example, modifications are made to reflect tense, case, aspect, person, number, gender and mood or position in speech. For example, googling fish (I use DuckDuckGo) will also result in fishes, fishing as fish is the stem of both words.\n",
    "\n",
    "Next we will utilise NLTK to perform stemming and lemmatisation for words in the DTM, to reduce the number of inflicted words.\n",
    "\n",
    "1. Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. A stem (or root) is the part of the word to which you add inflectional affixes such as (-ed,-ize, -s,-de,mis). They are obtained by removing the inflections used with a word.\n",
    "2. Lemmatization reduces the inflected words properly, ensuring that the root word belongs to the language. In Lemmatization the root word is called a Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. Context is an essential input for lemmatization, so the part of speech of the word (verb, noun, adjective, etc.....) should be passed as a parameter as well.\n",
    "\n",
    "I love stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\brianwu\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 312
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Using the GUI select the punkt model and wordnet corpora for download.\n",
    "nltk.download()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stemmings for troubling\ntroubl\ntroubl\n\n Stemmings for young\nyoung\nyounger\n\n Lemmatisations for trouble/ troubling\nn/a -  trouble\nverb -  trouble\nnoun -  troubling\nadj -  troubling\n\n Lemmatisations for done\nverb -  do\nnoun -  done\nadj -  done\n"
    }
   ],
   "source": [
    "# Using the PortStemmer, which is less aggressive, and less likely to yield non-real words. (lower chance of over-stemming)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\" Stemmings for troubling\")\n",
    "print(porter.stem(\"trouble\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "\n",
    "print(\"\\n Stemmings for young\")\n",
    "print(porter.stem(\"young\"))\n",
    "print(porter.stem(\"younger\"))\n",
    "\n",
    "# if not defined, the default \"pos\" is \"noun\"\n",
    "print(\"\\n Lemmatisations for trouble/ troubling\")\n",
    "print(\"n/a - \", wordnet_lemmatizer.lemmatize(\"trouble\"))\n",
    "print(\"verb - \", wordnet_lemmatizer.lemmatize(\"troubling\", pos = \"v\"))\n",
    "print(\"noun - \", wordnet_lemmatizer.lemmatize(\"troubling\", pos = \"n\"))\n",
    "print(\"adj - \", wordnet_lemmatizer.lemmatize(\"troubling\", pos = \"a\"))\n",
    "\n",
    "print(\"\\n Lemmatisations for done\")\n",
    "print(\"verb - \", wordnet_lemmatizer.lemmatize(\"done\", pos = \"v\"))\n",
    "print(\"noun - \", wordnet_lemmatizer.lemmatize(\"done\", pos = \"n\"))\n",
    "print(\"adj - \", wordnet_lemmatizer.lemmatize(\"done\", pos = \"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key observations can be made for both stemming and lemmatisation algorithms on NLTK:\n",
    "- Stemming still yields some non-real words ocassionally, which is something we will have to manually deal with when the word comes to the top of our list. \n",
    "- Lemmatisations for verbs seem to yield the most straightforward results, but we have to identify the parts of speech manually.\n",
    "\n",
    "This isn't quite what I am looking for, which is probably a signal for me to experiment with SpaCy as well.\n",
    "\n",
    "# Exploring SpaCy\n",
    "\n",
    "## Parts of Speech (POS) tagging, dependencies, entities and more\n",
    "\n",
    "With that in mind, perhaps we should experiment with spacy as well. For basic NLP tasks such as tokenization and lemmatisation (but not stemming), SpaCy offers a smaller variety of options but some say the functionality is more refined.\n",
    "\n",
    "(To be fair, getting the POS of a word is possible in NLTK as well as shown in this [article](https://www.tutorialexample.com/improve-nltk-word-lemmatization-with-parts-of-speech-nltk-tutorial/), but its not as straightfoward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spacy packages (install on computer via terminal/ anaconda, and load the language model)\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word         POS        Word Tag   Depend     Explain\n--------------------------------------------------------------------------------------------\nGrandpa      PROPN      NNP        compound   noun, proper singular\nJoe          PROPN      NNP        nsubj      noun, proper singular\nlifted       VERB       VBD        ccomp      verb, past tense\nCharlie      PROPN      NNP        dobj       noun, proper singular\nup           ADP        RP         prt        adverb, particle\nso           SCONJ      IN         mark       conjunction, subordinating or preposition\nthat         SCONJ      IN         mark       conjunction, subordinating or preposition\nhe           PRON       PRP        nsubj      pronoun, personal\ncould        VERB       MD         aux        verb, modal auxiliary\nget          AUX        VB         advcl      verb, base form\na            DET        DT         det        determiner\nbetter       ADJ        JJR        amod       adjective, comparative\nview         NOUN       NN         dobj       noun, singular or mass\n,            PUNCT      ,          punct      punctuation mark, comma\nand          CCONJ      CC         cc         conjunction, coordinating\nlooking      VERB       VBG        conj       verb, gerund or present participle\nin           ADV        RB         prt        adverb\n,            PUNCT      ,          punct      punctuation mark, comma\nCharlie      PROPN      NNP        nsubj      noun, proper singular\nsaw          VERB       VBD        ROOT       verb, past tense\na            DET        DT         det        determiner\nlong         ADJ        JJ         amod       adjective\ntable        NOUN       NN         dobj       noun, singular or mass\n,            PUNCT      ,          punct      punctuation mark, comma\nand          CCONJ      CC         cc         conjunction, coordinating\non           ADP        IN         prep       conjunction, subordinating or preposition\nthe          DET        DT         det        determiner\ntable        NOUN       NN         pobj       noun, singular or mass\nthere        PRON       EX         expl       existential there\nwere         AUX        VBD        conj       verb, past tense\nrows         NOUN       NNS        attr       noun, plural\nand          CCONJ      CC         cc         conjunction, coordinating\nrows         NOUN       NNS        conj       noun, plural\nof           ADP        IN         prep       conjunction, subordinating or preposition\nsmall        ADJ        JJ         amod       adjective\nwhite        ADJ        JJ         amod       adjective\nsquare       ADV        RB         amod       adverb\n-            PUNCT      HYPH       punct      punctuation mark, hyphen\nshaped       ADJ        JJ         amod       adjective\nsweets       NOUN       NNS        pobj       noun, plural\n.            PUNCT      .          punct      punctuation mark, sentence closer\n"
    }
   ],
   "source": [
    "# Basic Tasks - as usual we start with a (small) experiment.\n",
    "sentence = sp(u'Grandpa Joe lifted Charlie up so that he could get a better view, and looking in, Charlie saw a long table, and on the table there were rows and rows of small white square-shaped sweets.')\n",
    "\n",
    "\n",
    "# Print every word in sentence (which is now tokenized into numerous tokens), as the parts of speech (verb, noun, adjective etc.) or pos of each tokens, as well as its dependencies\n",
    "print(f'{\"Word\":{12}} {\"POS\":{10}} {\"Word Tag\":{10}} {\"Depend\":{10}} {\"Explain\"}')\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "for word in sentence:\n",
    "    # print(word.text,  word.pos_, word.dep_)\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{10}} {word.dep_:{10}} {spacy.explain(word.tag_)}')\n",
    "# simple print rewritten for a bit more clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"182178bd9d3c4f1aacd89bd8ede797dc-0\" class=\"displacy\" width=\"3110\" height=\"477.0\" direction=\"ltr\" style=\"max-width: none; height: 477.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Grandpa</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"135\">Joe</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"135\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"220\">lifted</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"220\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"305\">Charlie</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"305\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"390\">up</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"390\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"475\">so</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"475\">SCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"560\">that</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"560\">SCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"645\">he</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"645\">PRON</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"730\">could</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"730\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"815\">get</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"815\">AUX</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"900\">a</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"900\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"985\">better</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"985\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1070\">view,</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1070\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1155\">and</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1155\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1240\">looking</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1240\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1325\">in,</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1325\">ADV</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1410\">Charlie</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1410\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1495\">saw</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1495\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1580\">a</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1580\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1665\">long</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1665\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1750\">table,</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1750\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1835\">and</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1835\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1920\">on</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1920\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2005\">the</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2005\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2090\">table</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2090\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2175\">there</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2175\">PRON</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2260\">were</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2260\">AUX</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2345\">rows</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2345\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2430\">and</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2430\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2515\">rows</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2515\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2600\">of</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2600\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2685\">small</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2685\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2770\">white</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2770\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2855\">square-</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2855\">ADV</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2940\">shaped</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2940\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"387.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">sweets.</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-0\" stroke-width=\"2px\" d=\"M70,342.0 C70,299.5 100.0,299.5 100.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,344.0 L62,332.0 78,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-1\" stroke-width=\"2px\" d=\"M155,342.0 C155,299.5 185.0,299.5 185.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M155,344.0 L147,332.0 163,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-2\" stroke-width=\"2px\" d=\"M240,342.0 C240,2.0 1495.0,2.0 1495.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M240,344.0 L232,332.0 248,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-3\" stroke-width=\"2px\" d=\"M240,342.0 C240,299.5 270.0,299.5 270.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M270.0,344.0 L278.0,332.0 262.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-4\" stroke-width=\"2px\" d=\"M240,342.0 C240,257.0 360.0,257.0 360.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M360.0,344.0 L368.0,332.0 352.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-5\" stroke-width=\"2px\" d=\"M495,342.0 C495,172.0 795.0,172.0 795.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M495,344.0 L487,332.0 503,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-6\" stroke-width=\"2px\" d=\"M580,342.0 C580,214.5 790.0,214.5 790.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M580,344.0 L572,332.0 588,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-7\" stroke-width=\"2px\" d=\"M665,342.0 C665,257.0 785.0,257.0 785.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M665,344.0 L657,332.0 673,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-8\" stroke-width=\"2px\" d=\"M750,342.0 C750,299.5 780.0,299.5 780.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M750,344.0 L742,332.0 758,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-9\" stroke-width=\"2px\" d=\"M240,342.0 C240,87.0 805.0,87.0 805.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M805.0,344.0 L813.0,332.0 797.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-10\" stroke-width=\"2px\" d=\"M920,342.0 C920,257.0 1040.0,257.0 1040.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M920,344.0 L912,332.0 928,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-11\" stroke-width=\"2px\" d=\"M1005,342.0 C1005,299.5 1035.0,299.5 1035.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1005,344.0 L997,332.0 1013,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-12\" stroke-width=\"2px\" d=\"M835,342.0 C835,214.5 1045.0,214.5 1045.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1045.0,344.0 L1053.0,332.0 1037.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-13\" stroke-width=\"2px\" d=\"M835,342.0 C835,172.0 1135.0,172.0 1135.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1135.0,344.0 L1143.0,332.0 1127.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-14\" stroke-width=\"2px\" d=\"M835,342.0 C835,129.5 1225.0,129.5 1225.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1225.0,344.0 L1233.0,332.0 1217.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-15\" stroke-width=\"2px\" d=\"M1260,342.0 C1260,299.5 1290.0,299.5 1290.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1290.0,344.0 L1298.0,332.0 1282.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-16\" stroke-width=\"2px\" d=\"M1430,342.0 C1430,299.5 1460.0,299.5 1460.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1430,344.0 L1422,332.0 1438,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-17\" stroke-width=\"2px\" d=\"M1600,342.0 C1600,257.0 1720.0,257.0 1720.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1600,344.0 L1592,332.0 1608,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-18\" stroke-width=\"2px\" d=\"M1685,342.0 C1685,299.5 1715.0,299.5 1715.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1685,344.0 L1677,332.0 1693,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-19\" stroke-width=\"2px\" d=\"M1515,342.0 C1515,214.5 1725.0,214.5 1725.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1725.0,344.0 L1733.0,332.0 1717.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-20\" stroke-width=\"2px\" d=\"M1515,342.0 C1515,172.0 1815.0,172.0 1815.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1815.0,344.0 L1823.0,332.0 1807.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-21\" stroke-width=\"2px\" d=\"M1940,342.0 C1940,172.0 2240.0,172.0 2240.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1940,344.0 L1932,332.0 1948,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-22\" stroke-width=\"2px\" d=\"M2025,342.0 C2025,299.5 2055.0,299.5 2055.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2025,344.0 L2017,332.0 2033,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-23\" stroke-width=\"2px\" d=\"M1940,342.0 C1940,257.0 2060.0,257.0 2060.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2060.0,344.0 L2068.0,332.0 2052.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-24\" stroke-width=\"2px\" d=\"M2195,342.0 C2195,299.5 2225.0,299.5 2225.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2195,344.0 L2187,332.0 2203,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-25\" stroke-width=\"2px\" d=\"M1515,342.0 C1515,44.5 2255.0,44.5 2255.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2255.0,344.0 L2263.0,332.0 2247.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-26\" stroke-width=\"2px\" d=\"M2280,342.0 C2280,299.5 2310.0,299.5 2310.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2310.0,344.0 L2318.0,332.0 2302.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-27\" stroke-width=\"2px\" d=\"M2365,342.0 C2365,299.5 2395.0,299.5 2395.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-27\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2395.0,344.0 L2403.0,332.0 2387.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-28\" stroke-width=\"2px\" d=\"M2365,342.0 C2365,257.0 2485.0,257.0 2485.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-28\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2485.0,344.0 L2493.0,332.0 2477.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-29\" stroke-width=\"2px\" d=\"M2365,342.0 C2365,214.5 2575.0,214.5 2575.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-29\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2575.0,344.0 L2583.0,332.0 2567.0,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-30\" stroke-width=\"2px\" d=\"M2705,342.0 C2705,172.0 3005.0,172.0 3005.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-30\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2705,344.0 L2697,332.0 2713,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-31\" stroke-width=\"2px\" d=\"M2790,342.0 C2790,214.5 3000.0,214.5 3000.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-31\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2790,344.0 L2782,332.0 2798,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-32\" stroke-width=\"2px\" d=\"M2875,342.0 C2875,299.5 2905.0,299.5 2905.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-32\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2875,344.0 L2867,332.0 2883,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-33\" stroke-width=\"2px\" d=\"M2960,342.0 C2960,299.5 2990.0,299.5 2990.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-33\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2960,344.0 L2952,332.0 2968,332.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-34\" stroke-width=\"2px\" d=\"M2620,342.0 C2620,129.5 3010.0,129.5 3010.0,342.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-182178bd9d3c4f1aacd89bd8ede797dc-0-34\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M3010.0,344.0 L3018.0,332.0 3002.0,332.0\" fill=\"currentColor\"/>\n</g>\n</svg></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Visualising POS tags - neat!\n",
    "from spacy import displacy\n",
    "displacy.render(sentence, style='dep', jupyter=True, options={'distance': 85})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, named entities can be identified as well. This will be useful when we want to count the number of unique entities within the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word         Entity_Label   Explain\n-----------------------------------------\nGrandpa Joe  PERSON         None\nCharlie      PERSON         None\nCharlie      PERSON         None\n"
    }
   ],
   "source": [
    "print(f'{\"Word\":{12}} {\"Entity_Label\":{14}} {\"Explain\"}')\n",
    "print(\"-----------------------------------------\")\n",
    "for entity in sentence.ents:\n",
    "    print(f'{entity.text:{12}} {entity.label_:{14}} {str(spacy.explain(entity.label))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see, SpaCy has a much more advanced tagging algorithms for POS (`.pos`), entities (`.ents`) and also nouns(`.noun_chunks`), which account for the word's position within the sentence. I'm not gonna go too deep into this, but some of the POS identified are:\n",
    "- (various forms of) verbs\n",
    "- (singular or mass) nouns\n",
    "- (various forms of) pronouns\n",
    "- adjectives\n",
    "- auxilaries\n",
    "- conjunctions\n",
    "and more........... for a complete list of tags check out SpaCy's [annotation specifications](https://spacy.io/api/annotation#pos-tagging)\n",
    "\n",
    "I imagine the tagging algorithms will make things way easier for lemmatisation in SpaCy, since much more context is extracted with it's algorithm.\n",
    "\n",
    "## Lemmatisation on SpaCy\n",
    "\n",
    "Although stemming is not available on SpaCy, I believe that lemmatisation might actually yield slightly more useful results (which is we've seen with lemmatisation on NLTK as well), because the root lemma of a word can be returned, regardless of its part of speech, tense or more. That said, the assumption for correct lemmatisation results is that the part of speech tagging is good, which seems to be the case for SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word         Lemma \n-----------------------------------\nGrandpa      Grandpa\nJoe          Joe\nlifted       lift\nCharlie      Charlie\nup           up\nso           so\nthat         that\nhe           -PRON-\ncould        could\nget          get\na            a\nbetter       well\nview         view\n,            ,\nand          and\nlooking      look\nin           in\n,            ,\nCharlie      Charlie\nsaw          see\na            a\nlong         long\ntable        table\n,            ,\nand          and\non           on\nthe          the\ntable        table\nthere        there\nwere         be\nrows         row\nand          and\nrows         row\nof           of\nsmall        small\nwhite        white\nsquare       square\n-            -\nshaped       shaped\nsweets       sweet\n.            .\n"
    }
   ],
   "source": [
    "print(f'{\"Word\":{12}} {\"Lemma\"} ')\n",
    "print(\"-----------------------------------\")\n",
    "for word in sentence:\n",
    "    print(f'{word.text:{12}} {word.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new DTM with SpaCy\n",
    "\n",
    "With this in mind, I think I'm ready to redo my DTM using SpaCy from my original corpus `basic_cleaned_corpus.pkl`. With a few slight modifications to my agenda, the things to work on are:\n",
    "- Tokenization of sentences\n",
    "- Removal of stop words\n",
    "- Identifying entities\n",
    "- Lemmatisation, which is inherently better due to SpaCy's superior POS tagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                                                                                                                             text\nbook_names                                                                                                                                                       \nchocofact       here comes charlie   these two very old people are the father and mother of mr bucket  their names are  grandpa joe and grandma josephine    a...\nfox         down in the valley there were three farms  the owners of these farms had done well  they were rich men  they were also nasty men  all three of the...\nmatilda      the reader of books    it’s a funny thing about mothers and fathers  even  when their own child is the most disgusting little blister  you could ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>here comes charlie   these two very old people are the father and mother of mr bucket  their names are  grandpa joe and grandma josephine    a...</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>down in the valley there were three farms  the owners of these farms had done well  they were rich men  they were also nasty men  all three of the...</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>the reader of books    it’s a funny thing about mothers and fathers  even  when their own child is the most disgusting little blister  you could ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "source": [
    "cleaned_corpus = pd.read_pickle('basic_cleaned_corpus.pkl')\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization with SpaCy\n",
    "\n",
    "I'm being slightly greedy this time (working on both tokenization and lemmatization simultaneously) because looping through such a large library of texts repetitively is extremely time consuming....... there is a better way to this........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'come charlie old people father mother mr bucket name grandpa joe grandma josephine old people father mother mrs bucket name grandpa george grandma georgina mr bucket mrs bucket mr mrs bucket small boy charlie charlie d’you d’you d’you pleased meet family — grown up count little charlie bucket — live small wooden house edge great town house nearly large people life extremely uncomfortable room place altogether bed bed give old grandparent old tired tired get grandpa joe grandma josephine grandpa '"
     },
     "metadata": {},
     "execution_count": 320
    }
   ],
   "source": [
    "# Load tools for SpaCy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Create document with SpaCy annotations (POS, entities, dependencies etc.) and lemmatise immediately\n",
    "def createSpaCy(storyName):\n",
    "    data = sp(cleaned_corpus.text.loc[storyName])\n",
    "    new_data = []\n",
    "    for word in data:\n",
    "        if word.is_stop == False and not word.is_space: # only append if it's not a stop word.\n",
    "            new_data.append(word.lemma_)\n",
    "        sentence = \" \".join(new_data)\n",
    "    return sentence\n",
    "\n",
    "spacy_doc = [createSpaCy(storyName) for storyName in bookNames]\n",
    "spacy_doc[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   full_names  \\\nbook_names                                      \nchocofact   Charlie and the Chocolate Factory   \nfox                         Fantastic Mr Fox!   \nmatilda                               Matilda   \n\n                                                                                                                                                   spacified_text  \nbook_names                                                                                                                                                         \nchocofact   come charlie old people father mother mr bucket name grandpa joe grandma josephine old people father mother mrs bucket name grandpa george grandma...  \nfox         valley farm owner farm rich man nasty man nasty mean man meet name farmer boggis farmer bunce farmer bean boggi chicken farmer keep thousand chick...  \nmatilda     reader book funny thing mother father child disgusting little blister imagine think wonderful parent blinded adoration manage convince child quali...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_names</th>\n      <th>spacified_text</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>Charlie and the Chocolate Factory</td>\n      <td>come charlie old people father mother mr bucket name grandpa joe grandma josephine old people father mother mrs bucket name grandpa george grandma...</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>Fantastic Mr Fox!</td>\n      <td>valley farm owner farm rich man nasty man nasty mean man meet name farmer boggis farmer bunce farmer bean boggi chicken farmer keep thousand chick...</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>Matilda</td>\n      <td>reader book funny thing mother father child disgusting little blister imagine think wonderful parent blinded adoration manage convince child quali...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 393
    }
   ],
   "source": [
    "spacy_df = pd.DataFrame({'book_names':bookNames, 'full_names': fullNames, 'spacified_text': spacy_doc})\n",
    "spacy_df = spacy_df.set_index('book_names')\n",
    "\n",
    "# Keep a lemmatised version of the corpus\n",
    "spacy_df.to_pickle('lemmatised_books.pkl')\n",
    "spacy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tokenization and transforming the data into a document term matrix is easier with scikit-learn, I'm gonna process the sentences with `CountVectoriser()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            aback  abandon  abc  abdomen  abide  ability  able  absolute  \\\nbook_names                                                                 \nchocofact       0        0    0        0      1        0    13         2   \nfox             0        0    0        0      0        0     0         0   \nmatilda         1        1    1        1      0        4    14         5   \n\n            absolutely  absorb  ...  yippeeeeee  yippeeeeeeee  yippeeeeeeeeee  \\\nbook_names                      ...                                             \nchocofact           10       0  ...           1             1               1   \nfox                  1       0  ...           0             0               0   \nmatilda              6       3  ...           0             0               0   \n\n            you  youheard  young  youreally  youth  zing  zip  \nbook_names                                                     \nchocofact     7         0      7          0      1     1    1  \nfox           1         1      1          1      0     0    0  \nmatilda       3         0     15          0      0     0    0  \n\n[3 rows x 4382 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aback</th>\n      <th>abandon</th>\n      <th>abc</th>\n      <th>abdomen</th>\n      <th>abide</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>absolute</th>\n      <th>absolutely</th>\n      <th>absorb</th>\n      <th>...</th>\n      <th>yippeeeeee</th>\n      <th>yippeeeeeeee</th>\n      <th>yippeeeeeeeeee</th>\n      <th>you</th>\n      <th>youheard</th>\n      <th>young</th>\n      <th>youreally</th>\n      <th>youth</th>\n      <th>zing</th>\n      <th>zip</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>13</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>14</td>\n      <td>5</td>\n      <td>6</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 4382 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 322
    }
   ],
   "source": [
    "# Tokenize text and transform it in a DTM\n",
    "blank_cv = CountVectorizer()\n",
    "spacy_cv = blank_cv.fit_transform(spacy_df.spacified_text) \n",
    "\n",
    "#Transform DTM (in context a vocabulary/ dictionary) into pandas dataframe format\n",
    "spacy_dtm = pd.DataFrame(spacy_cv.toarray(), columns=blank_cv.get_feature_names())\n",
    "spacy_dtm.index = spacy_df.index\n",
    "spacy_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             say   mr  miss  matilda  honey   go  wonka  like  come  look  \\\nbook_names                                                                  \nchocofact    352  386    12        0      0  159    324   103   137   105   \nfox          152  132     2        0      0   39      0    23    32    31   \nmatilda      589   70   453      430    382  155      0   163   114   122   \nsum         1093  588   467      430    382  353    324   289   283   258   \n\n            ...  pad  murder  devising  lavatory  maggot  serpent  magician  \\\nbook_names  ...                                                               \nchocofact   ...    0       1         0         0       0        0         2   \nfox         ...    0       0         0         0       0        0         0   \nmatilda     ...    2       1         2         2       2        2         0   \nsum         ...    2       2         2         2       2        2         2   \n\n            messing  subtle  gasping  \nbook_names                            \nchocofact         2       0        0  \nfox               0       0        0  \nmatilda           0       2        2  \nsum               2       2        2  \n\n[4 rows x 2394 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>say</th>\n      <th>mr</th>\n      <th>miss</th>\n      <th>matilda</th>\n      <th>honey</th>\n      <th>go</th>\n      <th>wonka</th>\n      <th>like</th>\n      <th>come</th>\n      <th>look</th>\n      <th>...</th>\n      <th>pad</th>\n      <th>murder</th>\n      <th>devising</th>\n      <th>lavatory</th>\n      <th>maggot</th>\n      <th>serpent</th>\n      <th>magician</th>\n      <th>messing</th>\n      <th>subtle</th>\n      <th>gasping</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>352</td>\n      <td>386</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>159</td>\n      <td>324</td>\n      <td>103</td>\n      <td>137</td>\n      <td>105</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>152</td>\n      <td>132</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39</td>\n      <td>0</td>\n      <td>23</td>\n      <td>32</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>589</td>\n      <td>70</td>\n      <td>453</td>\n      <td>430</td>\n      <td>382</td>\n      <td>155</td>\n      <td>0</td>\n      <td>163</td>\n      <td>114</td>\n      <td>122</td>\n      <td>...</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>sum</th>\n      <td>1093</td>\n      <td>588</td>\n      <td>467</td>\n      <td>430</td>\n      <td>382</td>\n      <td>353</td>\n      <td>324</td>\n      <td>289</td>\n      <td>283</td>\n      <td>258</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 2394 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 391
    }
   ],
   "source": [
    "# Transpose for easier calculations, and add a column for number for sum of word frequencies in all books.\n",
    "filtered_spacy_dtm = spacy_dtm.transpose()\n",
    "filtered_spacy_dtm['sum'] = spacy_dtm.sum() \n",
    "filtered_spacy_dtm.sort_values(by= 'sum', inplace=True, ascending =False)\n",
    "\n",
    "# Only keep words that have appeared more than 5 times.\n",
    "filtered_spacy_dtm = filtered_spacy_dtm[filtered_spacy_dtm['sum'] > 1]\n",
    "filtered_spacy_dtm.sort_values(by= 'sum', inplace=True, ascending =False)\n",
    "\n",
    "#Transpose back to the original format and display the DTM!\n",
    "filtered_spacy_dtm = filtered_spacy_dtm.transpose()\n",
    "filtered_spacy_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering words that have appeared more than or equal to 2 times yield several advantages:\n",
    "- Less computing time during future manipulations\n",
    "- Minimises the chance of taking non-real words into the DTM.\n",
    "\n",
    "Now, let's pickle this for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_spacy_dtm.to_pickle(\"refined_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Idenitifaction with SpaCy\n",
    "\n",
    "Conversely, we would like to explore words which rarely occur, and to do so we identify the unique entities within the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['charlie',\n 'two',\n 'grandpa joe',\n 'grandma',\n 'two',\n 'georgina',\n 'charlie',\n 'charlie',\n 'six',\n 'only two',\n 'only one',\n 'four',\n 'grandpa joe',\n 'grandma',\n 'georgina',\n 'little charlie',\n 'winter',\n 'all night',\n 'one half',\n 'second',\n 'one',\n 'two',\n 'two',\n 'charlie',\n 'charlie',\n 'charlie',\n 'about from morning',\n 'one',\n 'charlie',\n 'charlie',\n 'the  great day',\n 'charlie',\n 'one',\n 'the next few days',\n 'one',\n 'the next day',\n 'charlie',\n 'more than a month',\n 'one',\n 'house',\n 'charlie',\n 'willy',\n 'half a mile',\n 'charlie',\n 'charlie',\n 'four',\n 'ninety',\n 'the day',\n 'charlie',\n 'one']"
     },
     "metadata": {},
     "execution_count": 326
    }
   ],
   "source": [
    "# Function to identify entities\n",
    "def identifyENT(storyName):\n",
    "    data = sp(cleaned_corpus.text.loc[storyName])\n",
    "    new_data = []\n",
    "    for entity in data.ents:\n",
    "        new_data.append(entity.text) # things to consider: entity.label_, spacy.explain(entity.label)\n",
    "    return new_data\n",
    "\n",
    "# Run the function and return the list (of list)\n",
    "entityList = [identifyENT(storyName) for storyName in bookNames]\n",
    "entityList[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oddly, this returned a bunch of numbers as well, so I'm gonna manually modify the function (a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['charlie',\n 'grandpa joe',\n 'grandma',\n 'georgina',\n 'charlie',\n 'charlie',\n 'only two',\n 'only one',\n 'grandpa joe',\n 'grandma',\n 'georgina',\n 'little charlie',\n 'winter',\n 'all night',\n 'one half',\n 'second',\n 'charlie',\n 'charlie',\n 'charlie',\n 'about from morning']"
     },
     "metadata": {},
     "execution_count": 379
    }
   ],
   "source": [
    "# Function to identify entities\n",
    "def identifyENT_revised(storyName):\n",
    "    data = sp(cleaned_corpus.text.loc[storyName])\n",
    "    new_data = []\n",
    "    for entity in data.ents:\n",
    "        if entity.text != \"one\" and entity.text != \"two\" and entity.text != \"three\" and entity.text != \"four\" and entity.text != \"five\" and entity.text != \"six\" and entity.text != \"seven\" and entity.text != \"eight\" and entity.text != \"nine\" and entity.text != \"ten\":\n",
    "            new_data.append(entity.text) # things to consider: entity.label_, spacy.explain(entity.label)\n",
    "    return new_data\n",
    "\n",
    "# Run the function and return the list (of list)\n",
    "entityList = [identifyENT_revised(storyName) for storyName in bookNames]\n",
    "entityList[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                   full_names  \\\nbook_names                                      \nchocofact   Charlie and the Chocolate Factory   \nfox                         Fantastic Mr Fox!   \nmatilda                               Matilda   \n\n                                                                                                                                                         entities  \nbook_names                                                                                                                                                         \nchocofact   [charlie, grandpa joe, grandma, georgina, charlie, charlie, only two, only one, grandpa joe, grandma, georgina, little charlie, winter, all night,...  \nfox         [thousands, thousands, turkey, apple, thousands, gallons, valley, fox, fox, fox, fox, turkey, fox, fox, valley, fox, fox, fox, fifty yards, number...  \nmatilda     [spring, six years, no more than six days, michael, matilda, matilda, the next county, half¬, michael, the age of one and a half, twelve inch, fiv...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_names</th>\n      <th>entities</th>\n    </tr>\n    <tr>\n      <th>book_names</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>chocofact</th>\n      <td>Charlie and the Chocolate Factory</td>\n      <td>[charlie, grandpa joe, grandma, georgina, charlie, charlie, only two, only one, grandpa joe, grandma, georgina, little charlie, winter, all night,...</td>\n    </tr>\n    <tr>\n      <th>fox</th>\n      <td>Fantastic Mr Fox!</td>\n      <td>[thousands, thousands, turkey, apple, thousands, gallons, valley, fox, fox, fox, fox, turkey, fox, fox, valley, fox, fox, fox, fifty yards, number...</td>\n    </tr>\n    <tr>\n      <th>matilda</th>\n      <td>Matilda</td>\n      <td>[spring, six years, no more than six days, michael, matilda, matilda, the next county, half¬, michael, the age of one and a half, twelve inch, fiv...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 328
    }
   ],
   "source": [
    "# As usual we are going to do a bunch of manipulation to change it into a DTM form.\n",
    "\n",
    "entity_df = pd.DataFrame({'book_names':bookNames, 'full_names': fullNames, 'entities': entityList})\n",
    "entity_df = entity_df.set_index('book_names')\n",
    "entity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "w days': 1,\n          'the next day': 2,\n          'more than a month': 1,\n          'house': 4,\n          'willy': 12,\n          'half a mile': 1,\n          'ninety': 1,\n          'the day': 4,\n          'heard charlie': 1,\n          'grandpa': 8,\n          'george': 2,\n          'all day long': 3,\n          'half an hour': 3,\n          'one evening': 1,\n          'about fifty': 2,\n          'little charlie    ': 1,\n          'ninety six and a half': 2,\n          'more than two hundred': 1,\n          'earth': 5,\n          'cold for hours and hours': 1,\n          'all morning': 1,\n          'every ten seconds': 1,\n          'indian': 2,\n          'prince pondicherry ’': 1,\n          'grandpa george  ': 4,\n          'india': 1,\n          'one hundred': 4,\n          'charlie    ‘': 2,\n          'grandpa joe    ': 12,\n          'charlie    ': 6,\n          'tonight': 1,\n          'tomorrow': 4,\n          'evening': 7,\n          'the next evening': 1,\n          'thousands': 4,\n          'one day': 2,\n          'fickelgruber': 2,\n          'prodnose': 1,\n          'months and': 1,\n          'months': 1,\n          'morning': 3,\n          'charlie quickly': 1,\n          'every day': 2,\n          'late at night': 1,\n          'the last ten  years': 1,\n          'today': 5,\n          'this year': 1,\n          'grandma josephine': 1,\n          'next week': 1,\n          'grandpa george': 2,\n          'first': 19,\n          'next day': 1,\n          'nine year old': 1,\n          'russia': 1,\n          'charlotte russe': 1,\n          'english': 3,\n          'the day before': 1,\n          'hundreds of thousands': 1,\n          'about a hundred': 1,\n          'till night': 1,\n          'three days': 1,\n          'each day': 1,\n          'hours': 1,\n          'the evening of the fourth day': 1,\n          'charlie  ': 2,\n          'third': 4,\n          'next morning': 1,\n          'josephine': 2,\n          'only three': 1,\n          'the middle       ': 1,\n          'fourth': 3,\n          'violet beauregarde': 5,\n          'a few minutes': 1,\n          'every minute of the day': 1,\n          'beauregarde': 2,\n          'three months': 2,\n          'mike teavee': 37,\n          'teavee': 22,\n          'the nine year old': 1,\n          'half a dozen': 2,\n          'grandpa george    ': 2,\n          'joe': 4,\n          'five  minutes': 1,\n          'the next two weeks': 1,\n          'one morning': 1,\n          'charlie bucket': 3,\n          'four feet': 1,\n          'days': 2,\n          'half': 5,\n          'several minutes': 1,\n          'mornings': 1,\n          'ten  minutes earlier': 1,\n          'one afternoon': 1,\n          'kerb': 1,\n          'bent': 1,\n          'fifty pence': 4,\n          'less than half': 1,\n          'five penny': 1,\n          'just one': 3,\n          'the wrapper       ': 1,\n          'a few seconds': 1,\n          'about twenty': 1,\n          'twenty': 4,\n          'weeks': 2,\n          'fifty pounds': 1,\n          'two hundred': 1,\n          'willy wonka': 1,\n          'five minutes': 1,\n          'yelled charlie': 1,\n          'fifth': 3,\n          'about ten seconds': 1,\n          'yippeeeeeeee': 1,\n          'these last twenty years': 1,\n          'yippeeeeeeeeee': 1,\n          'many years': 1,\n          'the day of  ': 1,\n          'the  first day in the month of february': 1,\n          'this day': 1,\n          'ten o’clock': 2,\n          'the first day of february': 1,\n          'the last  day of january': 1,\n          'polish': 1,\n          'a whole day': 1,\n          'hallelujah': 1,\n          'several hours': 1,\n          'midnight': 1,\n          'sun': 1,\n          'the morning': 1,\n          'veruca salt ’': 1,\n          'up half a million': 1,\n          'skinny': 1,\n          'veruca salt': 9,\n          'only yesterday': 1,\n          'this morning': 1,\n          'breakneck': 1,\n          'fourteen': 1,\n          'a dozen': 2,\n          'thousands of gallons an hour': 1,\n          'thousands and thousands of gallons': 1,\n          'veruca': 8,\n          'more than two': 1,\n          'oompa loompas': 5,\n          'cacao  ': 1,\n          'all day': 3,\n          'billions': 1,\n          'every week': 1,\n          'the tree house belonging': 1,\n          'cacao': 4,\n          'oompa': 5,\n          'loompa': 5,\n          'the tree house': 1,\n          'veruca    ': 1,\n          'augustus': 4,\n          'million': 3,\n          'grandpa joe    ‘': 1,\n          'five seconds': 2,\n          'strawberry flavoured': 2,\n          'fudge       ’': 1,\n          'tomorrow morning': 1,\n          'scott': 1,\n          'marbles': 1,\n          'a minute more': 1,\n          'at least ten': 1,\n          'little charlie bucket': 1,\n          'grandpa joe    charlie': 1,\n          'whips': 2,\n          'the   river': 1,\n          'three minutes': 1,\n          'kettles': 1,\n          'christmas': 1,\n          'phut': 3,\n          'phut phut phut': 1,\n          'gob': 1,\n          'beauregarde       ': 1,\n          'nearly a year': 1,\n          'yesterday': 1,\n          'bald heads': 1,\n          'hundreds and hundreds': 1,\n          'runny stuff': 1,\n          'hundreds': 2,\n          'mnny': 1,\n          'whizzer': 2,\n          'grey cardboard': 1,\n          'veruca salt    ': 5,\n          'violet': 1,\n          'a minute': 1,\n          'years': 3,\n          'fifty bits': 1,\n          'one summer': 1,\n          'the night': 1,\n          'the corridor    ': 1,\n          'cold days': 1,\n          'fizzy': 1,\n          'burp': 1,\n          'white square': 1,\n          'veruca  ': 2,\n          'my dear old fish ’': 1,\n          'lords': 1,\n          'nut': 4,\n          'glass': 2,\n          'charlie   ‘squirrels': 1,\n          'crikey': 1,\n          'mummy': 1,\n          'twenty five': 3,\n          'twenty four': 1,\n          'mbbish': 1,\n          'salt       ': 1,\n          'mike  ': 1,\n          'nut room': 1,\n          'hello  good morning  ': 1,\n          'a mile': 1,\n          'a thousand': 1,\n          'another thousand': 1,\n          'a month': 1,\n          'rainbow drops': 1,\n          'button': 1,\n          'grandpa joe’s': 2,\n          'brown sticky': 1,\n          'white powder': 1,\n          'no more than four inches': 1,\n          'holy mackerel': 1,\n          'wonka': 2,\n          '‘just a minute': 1,\n          'millions': 3,\n          'a million': 3,\n          'mike': 6,\n          'about half': 1,\n          'last week': 1,\n          'only half': 1,\n          'only a half': 1,\n          'ear to ear': 1,\n          'mum': 1,\n          'fro': 1,\n          'more than an inch': 1,\n          'maybe miles': 1,\n          'tittle charlie': 1,\n          'about a week': 1,\n          'charlie left    ': 1,\n          'swung': 1,\n          'grandpa joe   ': 1,\n          'five  seconds': 1,\n          'a thousand feet': 1,\n          'one million': 1,\n          'about ten feet': 1,\n          'little charlie     ': 1,\n          'the end of the day': 1}),\n Counter({'thousands': 3,\n          'turkey': 4,\n          'apple': 1,\n          'gallons': 1,\n          'valley': 2,\n          'fox': 171,\n          'fifty yards': 1,\n          'number four': 1,\n          'dang': 4,\n          'earth': 7,\n          'tomorrow': 2,\n          'first': 5,\n          'tonight': 2,\n          'a mile': 2,\n          'inch': 1,\n          'half': 2,\n          'night': 4,\n          'three days': 5,\n          'a couple of hours': 1,\n          'miles': 1,\n          'second': 3,\n          'mummy': 2,\n          'all four': 1,\n          'about an hour': 2,\n          'sun': 1,\n          'the next morning': 1,\n          'five minutes': 2,\n          'only a few feet': 1,\n          'a few moments': 1,\n          'five o’clock in the afternoon': 1,\n          'six o’clock in the evening': 1,\n          'all day': 1,\n          'wedon’t': 1,\n          'two gallons': 1,\n          'thirty five': 1,\n          'thirty six': 1,\n          'thirty seven': 1,\n          'one hundred and eight': 2,\n          'the next day': 2,\n          'three nights': 1,\n          'the third day': 1,\n          'nights': 1,\n          'no days': 1,\n          'no nights': 1,\n          'thousand': 1,\n          'number one': 3,\n          'us': 2,\n          'geese': 1,\n          'thousands and thousands': 1,\n          'at least a hundred': 1,\n          'fifty': 1,\n          'saucy beast ’': 1,\n          'hundreds': 1,\n          'all night': 1,\n          'between two': 1,\n          'two or three': 1,\n          'yesterday': 1,\n          'today': 1,\n          'more than a few hours': 1,\n          'another day': 1,\n          'mabel': 1,\n          'cellar': 1,\n          'gallon': 1,\n          'burglars': 1,\n          'no less than twenty nine': 1,\n          'several days': 1,\n          'this day': 1,\n          'the days': 1,\n          'many minutes': 1}),\n Counter({'spring': 1,\n          'six years': 1,\n          'no more than six days': 1,\n          'michael': 6,\n          'matilda': 366,\n          'the next county': 1,\n          'half¬': 1,\n          'the age of one and a half': 1,\n          'twelve inch': 1,\n          'five years': 1,\n          'eight miles': 1,\n          'five afternoons': 1,\n          'the afternoon': 1,\n          'only ten minutes': 1,\n          'two glorious hours': 1,\n          'the past few  weeks': 1,\n          'years': 3,\n          'three months': 1,\n          'a four year old': 1,\n          'first': 32,\n          'fifteen year old': 1,\n          'last  ': 1,\n          'charles  ': 1,\n          'dickens': 4,\n          'hour  after hour': 1,\n          'dark¬': 1,\n          'ten to five': 1,\n          'the first week': 2,\n          'every day': 1,\n          'a week': 4,\n          'eleven': 1,\n          'the next six months': 1,\n          'nicholas nickleby': 3,\n          'charles dickens': 3,\n          'charlotte': 1,\n          'jane austen': 1,\n          'thomas hardy': 1,\n          'mary webb': 1,\n          'kim': 1,\n          'ernest hemingway': 2,\n          'william faulkner': 1,\n          'john steinbeck': 2,\n          'graham greene': 1,\n          'george orwell': 1,\n          'proba¬': 1,\n          'hemingway': 1,\n          'gould': 2,\n          'two weeks': 1,\n          'more than one': 1,\n          'out¬': 1,\n          'intro¬': 1,\n          'olden day': 1,\n          'joseph': 1,\n          'conrad': 1,\n          'africa': 1,\n          'india': 1,\n          'english': 3,\n          'roald dahl': 6,\n          'second': 16,\n          'second¬': 1,\n          'mike': 2,\n          'nut': 2,\n          'about a hundred  miles': 1,\n          'about a hundred': 1,\n          'fifty thousand miles': 2,\n          'speedo¬  meter': 1,\n          'ten years ago': 1,\n          'one hundred and fifty thousand': 1,\n          'only ten thousand': 1,\n          'thousands and thousands': 1,\n          'penicillin': 1,\n          'ten thousand': 1,\n          'american': 3,\n          'mummy': 5,\n          'five years old': 1,\n          'the following morning': 1,\n          'jay': 1,\n          'that evening': 1,\n          'half': 8,\n          'yesterday': 5,\n          'about  an hour': 1,\n          'the morning': 2,\n          'about a week': 1,\n          'a bad day': 1,\n          'wormwood': 6,\n          'fred': 3,\n          'wonder¬': 1,\n          'marvel¬': 1,\n          'one night': 2,\n          'next week': 1,\n          'a few  seconds': 2,\n          'tomorrow': 3,\n          'harry': 1,\n          'house': 9,\n          'rattle': 2,\n          'the next afternoon': 1,\n          'five year old': 3,\n          'weekday': 1,\n          'afternoons': 2,\n          'when¬': 1,\n          'several days': 1,\n          'over a week': 1,\n          'this evening': 1,\n          'day': 2,\n          'tonight': 1,\n          'this morning': 5,\n          'no less than five': 1,\n          'the end of each day': 1,\n          'two hundred and seventy eight': 1,\n          'one thousand four hundred and twenty five': 1,\n          'ten year old': 3,\n          'one  hundred and eighteen pounds': 1,\n          'seven  hundred and sixty': 1,\n          'one hundred and eleven': 1,\n          'nine hundred and ninety nine': 1,\n          'fifty pence': 2,\n          'nine hundred and ninety': 1,\n          'one thousand pounds': 1,\n          'nine  hundred and ninety nine fifty': 1,\n          'six pounds': 1,\n          'six hundred': 2,\n          'nine pounds': 1,\n          'six hundred and thirty seven': 1,\n          'sixteen hundred and forty nine': 1,\n          'fifty': 3,\n          'today': 6,\n          'less than ten  minutes': 1,\n          'four  thousand three hundred': 1,\n          'four thousand three  hundred': 1,\n          'four thousand': 1,\n          'three hundred and three pounds': 1,\n          'evening': 3,\n          'the next morning': 2,\n          'hair¬': 1,\n          'month': 1,\n          'shakespeare': 2,\n          'kept': 1,\n          'morning': 5,\n          'daily': 1,\n          'the early morning': 1,\n          'three quarters': 2,\n          'dining¬': 1,\n          'strawberry jam': 3,\n          'secondly': 1,\n          'noisy scene': 1,\n          'the entire circus season': 1,\n          'earth': 3,\n          'elizabeth': 1,\n          'arrange¬': 1,\n          'five and a half': 2,\n          'crunchem hall primary school': 1,\n          'about two hundred and fifty': 1,\n          'twelve years old': 1,\n          'establish¬': 1,\n          'eighteen': 5,\n          'more than  ': 1,\n          'twenty three or twenty four': 1,\n          'madonna': 1,\n          'jennifer honey': 2,\n          'class¬': 2,\n          'at least one': 1,\n          'bush': 1,\n          'the very first day': 1,\n          'crunchem hall': 2,\n          'the end of this week': 1,\n          'twelve': 3,\n          'only one': 1,\n          'the second  row': 1,\n          'twice twelve': 1,\n          'twenty four': 1,\n          'thirteen': 1,\n          'twenty six': 1,\n          'twenty eight': 2,\n          'thirty': 2,\n          'sixteen': 2,\n          'fifty six': 1,\n          'four hundred and eighty seven': 1,\n          'four hundred and eighty': 1,\n          'nine hundred and seventy four': 1,\n          'eighty four': 1,\n          'listen¬': 1,\n          'fourteen': 8,\n          'nineteen': 2,\n          'two hundred and sixty six': 2,\n          'polish': 2,\n          'honey': 34,\n          'a hundred years': 1,\n          'mozart': 1,\n          'com¬': 2,\n          'nigel': 21,\n          'five year olds': 2,\n          'thirty seconds': 2,\n          'jenny   ': 1,\n          'third': 4,\n          'matilda    ': 1,\n          'pickwick': 1,\n          'bell': 1,\n          'sinewy': 1,\n          'extra¬': 1,\n          'ten  thousand miles': 1,\n          'this  morning': 1,\n          'grin¬': 1,\n          'knickers': 1,\n          'only half an hour': 1,\n          'immedi¬': 1,\n          'the eleven year olds ’': 1,\n          'scott': 1,\n          'a little five year old': 1,\n          'a month ’': 1,\n          'play¬': 1,\n          'french': 1,\n          'between nine and ten': 1,\n          'nosey cook': 1,\n          'responsi¬': 1,\n          'a  few minutes': 1,\n          'angelica': 1,\n          'miss honey': 3,\n          'first day': 1,\n          'abc': 1,\n          'two or three years': 1,\n          'this minute': 1,\n          'five and a half year old': 1,\n          'the  first day': 1,\n          'over¬': 2,\n          'the first day': 1,\n          'the lunch hour': 1,\n          'the third day': 1,\n          'hortensia': 31,\n          'her private quarters': 1,\n          'only ten inches': 1,\n          'thou¬': 1,\n          'a whole day': 1,\n          'two hours': 2,\n          'a few seconds': 2,\n          'all day long': 1,\n          'sixth': 1,\n          'fifty  pence': 1,\n          'a few days later': 1,\n          'three feet': 1,\n          'two inches': 1,\n          'one day': 2,\n          'a  week': 1,\n          'julius': 2,\n          'julius rottwinkle': 1,\n          'remem': 1,\n          'britain': 1,\n          'olympics': 2,\n          'the red sea': 1,\n          'breeches': 1,\n          'the day': 4,\n          'my m m mummy': 2,\n          'amanda': 3,\n          'pigtail': 1,\n          'a million years': 1,\n          'mad': 1,\n          'next day': 1,\n          'two hundred and fifty or so': 1,\n          'eleven year old': 1,\n          'mafia': 1,\n          'bruce bogtrotter': 1,\n          'morn¬': 1,\n          'one minute': 1,\n          'cook': 1,\n          'china': 2,\n          'eighteen inches': 1,\n          'stringy cook': 1,\n          'ten seconds': 1,\n          'all day': 1,\n          'two     hundred and fifty': 1,\n          'long¬': 1,\n          'blazes': 2,\n          'the middle of the first week': 1,\n          'thursday': 2,\n          'these last few  days': 1,\n          'a dozen': 1,\n          'wellington': 1,\n          'about six inches': 2,\n          'amphibian': 1,\n          'the next day': 3,\n          'weekly': 1,\n          'afternoon': 2,\n          'the next six years': 1,\n          'the second row': 2,\n          'nigel hicks': 3,\n          'nigel went    ': 1,\n          'three minutes': 2,\n          'ffi': 1,\n          'lty': 1,\n          'rupert': 4,\n          'rupert suddenly': 1,\n          'eric ink': 1,\n          'eric': 10,\n          'rub': 1,\n          'a few minutes': 1,\n          'the last couple of minutes': 1,\n          'a week ago': 1,\n          'the  second row': 1,\n          'forty years': 1,\n          'some¬': 1,\n          'millions': 3,\n          'invis¬': 1,\n          'a fraction of an inch': 1,\n          'the last five minutes': 1,\n          'honev': 1,\n          'only two': 1,\n          'gendy    ': 1,\n          'jesus': 1,\n          'about ten feet': 2,\n          'the millions': 1,\n          'twelve inches': 1,\n          'see¬': 1,\n          'the first morning': 1,\n          'a couple of minutes': 1,\n          'marvelled': 1,\n          'winter': 1,\n          'dylan thomas': 1,\n          'wolf': 1,\n          'matilda hung': 1,\n          'fright¬': 1,\n          'a half': 1,\n          'lo': 1,\n          'bath': 1,\n          'england': 1,\n          'saucepan': 1,\n          'the next  morning': 1,\n          'sponge¬': 1,\n          'han': 1,\n          'two years ago': 2,\n          'twenty three years old': 1,\n          'australia': 1,\n          'the years': 1,\n          'house¬': 1,\n          'only forty minutes': 1,\n          'up to thousands': 1,\n          'the  next ten years': 1,\n          'one pound': 1,\n          'you’re mad': 1,\n          'ten pence': 1,\n          'one month': 1,\n          'forty pence': 1,\n          'one  pound': 1,\n          'two years ’': 1,\n          'ten  pence': 1,\n          'the  winter': 1,\n          'the  house': 1,\n          'about a foot': 1,\n          'this afternoon': 3,\n          'good¬': 1,\n          'about an inch': 1,\n          'about  ten seconds': 1,\n          'the next hour': 1,\n          'about a minute': 1,\n          'the  evening': 1,\n          'wednesday': 1,\n          'last thursday': 1,\n          'last week': 1,\n          'the last week ’': 1,\n          'unfort¬': 1,\n          'upv': 1,\n          'twenty one': 1,\n          'eight nutty': 1,\n          'twenty': 1,\n          'nigel    ': 1,\n          'tny tehny kcr    ': 1,\n          'nigel proudly    ': 1,\n          'matron': 1,\n          'later that day': 1,\n          'trilby': 3,\n          'investi¬': 1,\n          'the second morning': 1,\n          'the red house': 4,\n          'every single evening': 1,\n          'one evening': 1,\n          'a few weeks later': 1,\n          'jam': 1,\n          'three  hundred': 1,\n          'forty': 1,\n          'an hour': 1,\n          'about an eight': 1,\n          'mercedes motor': 1,\n          'half an hour': 1,\n          'spain': 5,\n          'thirty minutes': 1,\n          'less  than': 1,\n          'four minutes': 1,\n          'about thirty minutes': 1,\n          'number¬': 1,\n          'mercedes': 1,\n          'one’ll': 1,\n          'a penny': 1,\n          'suit¬': 1})]"
     },
     "metadata": {},
     "execution_count": 373
    }
   ],
   "source": [
    "import collections as coll\n",
    "\n",
    "entityCounter = []\n",
    "for storyNames in bookNames:\n",
    "    entityCounter.append(coll.Counter(entity_df.entities.loc[storyNames]))\n",
    "entityCounter #Results hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as we can see, this algorithm yields a lot of irrelevant results, a lot of which are related to time (hours/ seconds/ seasons......) I have a feeling that counting the number of entities wouldn't do much benefit to this analysis, so I'm gonna stop this section here right now. In addition, it would rquire quite a bit of manipulation to convert this list of lists (array) to list of counters and then to a dictionary; we can't use CountVectoriser because the number of words of each entity is not determined by CountVectoriser itself.\n",
    "\n",
    "The time is now to proceed to exploratory data anlaysis."
   ]
  }
 ]
}