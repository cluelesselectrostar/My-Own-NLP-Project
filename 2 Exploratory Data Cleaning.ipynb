{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Before applying any fancy algorithms, the next step is to simply explore whether the processed corpuses and DTMs make sense.\n",
    "\n",
    "When working with numerical data, some EDA techniques we can use include finding the mean, median or mode as well as the distribution of a data set. \n",
    "\n",
    "For text, we are going to find some more **obvious** patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. Relatively obious things are:\n",
    "\n",
    "1. **Most common words** - find these and create word clouds\n",
    "2. **Size of vocabulary** - look number of unique words\n",
    "\n",
    "## Outline for EDA\n",
    "1. With the DTM from the previous stage, sort into columns according to ascending order in value (of occurence)\n",
    "2. Aggregate (or filter) data - select columns with the largest values.\n",
    "3. Visualise top words - word clouds? bar charts?\n",
    "4. Insights - a written comment/ description of the key takeaways.\n",
    "5. Probably also try using TF-IDF (Term frequency - inverse data frequency) for better understanding the \"uniqueness\" or the \"value\" of the the vocabulary in the text."
   ]
  }
 ]
}